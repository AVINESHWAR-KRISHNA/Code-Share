import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions.{md5, sha2, concat_ws}

// Initialize Spark session
val spark = SparkSession.builder
  .appName("DataFrameComparison")
  .config("spark.sql.shuffle.partitions", "2000")
  .config("spark.executor.memory", "8g")
  .config("spark.driver.memory", "8g")
  .config("spark.network.timeout", "600s")
  .config("spark.executor.heartbeatInterval", "100s")
  .getOrCreate()

// JDBC connection properties
val jdbcUrl = "jdbc:sqlserver://your_sql_server:1433;databaseName=your_database"
val connectionProperties = new java.util.Properties()
connectionProperties.put("user", "your_username")
connectionProperties.put("password", "your_password")
connectionProperties.put("driver", "com.microsoft.sqlserver.jdbc.SQLServerDriver")

// Read data from SQL Server
val df1 = spark.read.jdbc(jdbcUrl, "your_table_name", connectionProperties)

// Read data from Data Lake (e.g., Parquet file)
val df2 = spark.read.parquet("path_to_your_datalake_file")

// Create a hash for each row
val df1Hashed = df1.withColumn("row_hash", md5(concat_ws("||", df1.columns.map(col): _*)))
val df2Hashed = df2.withColumn("row_hash", md5(concat_ws("||", df2.columns.map(col): _*)))

// Aggregate the row hashes
val df1AggHash = df1Hashed.agg(sha2(concat_ws("||", df1Hashed.select("row_hash").rdd.map(_.getString(0)).collect(): _*), 256)).collect()(0)(0)
val df2AggHash = df2Hashed.agg(sha2(concat_ws("||", df2Hashed.select("row_hash").rdd.map(_.getString(0)).collect(): _*), 256)).collect()(0)(0)

// Compare the hashes
if (df1AggHash == df2AggHash) {
  println("DataFrames are identical")
} else {
  println("DataFrames are different")

  // Find mismatched rows
  val mismatchedRows = df1Hashed.join(df2Hashed, Seq("row_hash"), "outer")
    .filter(df1Hashed("row_hash").isNull || df2Hashed("row_hash").isNull)

  // Show mismatched rows
  mismatchedRows.show()

  // Return mismatched rows as a DataFrame
  val mismatchedDF = mismatchedRows
}
